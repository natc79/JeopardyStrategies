{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies for Winning on Jeopardy\n",
    "\n",
    "This analysis draws on a data file containing close to 20K questions and answers that have been asked on Jeopardy shows over time.  The objective is to clean and parse the data to figure out whether there are any potential strategies for studying questions that could enhance the money one wins given entry onto a jeopardy show. The analysis looks at the prevalence of key words and phrases to answer questions such as:  How often are answers part of the questions?  Are there certain types of questions that are associated with higher value monetary amounts and therefore would suggest a focused strategy for studying?  How often have questions from past shows shown up in current ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Show Number    Air Date      Round                         Category  Value  \\\n",
      "0         4680  2004-12-31  Jeopardy!                          HISTORY   $200   \n",
      "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
      "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
      "3         4680  2004-12-31  Jeopardy!                 THE COMPANY LINE   $200   \n",
      "4         4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
      "\n",
      "                                            Question      Answer  \n",
      "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
      "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
      "2  The city of Yuma in this state has a record av...     Arizona  \n",
      "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  \n",
      "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  \n",
      "Index(['Show Number', ' Air Date', ' Round', ' Category', ' Value',\n",
      "       ' Question', ' Answer'],\n",
      "      dtype='object')\n",
      "Show Number\n",
      " Air Date\n",
      " Round\n",
      " Category\n",
      " Value\n",
      " Question\n",
      " Answer\n",
      "Index(['Show Number', 'Air Date', 'Round', 'Category', 'Value', 'Question',\n",
      "       'Answer'],\n",
      "      dtype='object')\n",
      "(19999, 7)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "jeopardy = pd.read_csv(\"jeopardy.csv\")\n",
    "print(jeopardy.head(5))\n",
    "print(jeopardy.columns)\n",
    "newlabels = []\n",
    "#Clean the column names and remove leading white spaces\n",
    "for i in jeopardy.columns:\n",
    "    print(i)\n",
    "    newlabels.append(i.lstrip())\n",
    "\n",
    "jeopardy.columns = newlabels\n",
    "print(jeopardy.columns)\n",
    "print(jeopardy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    for the last 8 years of his life galileo was u...\n",
      "1    no 2 1912 olympian; football star at carlisle ...\n",
      "2    the city of yuma in this state has a record av...\n",
      "3    in 1963 live on the art linkletter show this c...\n",
      "4    signer of the dec of indep framer of the const...\n",
      "Name: clean_question, dtype: object\n",
      "0    copernicus\n",
      "1    jim thorpe\n",
      "2       arizona\n",
      "3     mcdonalds\n",
      "4    john adams\n",
      "Name: clean_answer, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#This code develops a function to normalize questions and take a string,\n",
    "#convert to lowercase, remove punctuation and return the string\n",
    "\n",
    "punctuation = [\"'\",'''\"''',\".\",\"(\",\")\",\",\",\":\"]\n",
    "other_clean = [\"target=_blank>\",\"</a>\"]\n",
    "def normalize_text(string):\n",
    "    textstr = str.lower(string)\n",
    "    for i in punctuation:\n",
    "        textstr = str.replace(textstr,i,\"\")\n",
    "    for i in other_clean:\n",
    "        textstr = str.replace(textstr,i,\"\")\n",
    "    return(textstr)\n",
    "\n",
    "jeopardy[\"clean_question\"] = jeopardy[\"Question\"].apply(normalize_text)\n",
    "jeopardy[\"clean_answer\"] = jeopardy[\"Answer\"].apply(normalize_text)\n",
    "print(jeopardy[\"clean_question\"].head(5))\n",
    "print(jeopardy[\"clean_answer\"].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    200\n",
      "1    200\n",
      "Name: clean_value, dtype: int64\n",
      "0   2004-12-31\n",
      "1   2004-12-31\n",
      "Name: datetime, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "#Normalize dollar values\n",
    "punctuation = [\"$\",\".\",\",\"]\n",
    "def normalize_dollarvalues(string):\n",
    "    textstr = string\n",
    "    for i in punctuation:\n",
    "        textstr = str.replace(textstr,i,\"\")\n",
    "    try:\n",
    "        value = int(textstr)\n",
    "    except ValueError:\n",
    "        value = 0\n",
    "    return(value)\n",
    "\n",
    "jeopardy[\"clean_value\"]=jeopardy[\"Value\"].apply(normalize_dollarvalues)\n",
    "print(jeopardy[\"clean_value\"].head(2))\n",
    "\n",
    "jeopardy[\"datetime\"] = pd.to_datetime(jeopardy[\"Air Date\"], yearfirst=True)\n",
    "print(jeopardy[\"datetime\"].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of answers that were in questions: 5.83\n"
     ]
    }
   ],
   "source": [
    "#How often is the answer deducible from the question?\n",
    "#How often are new questions a repeat of older questions?\n",
    "\n",
    "def jeopardyrow(series):\n",
    "    split_answer = series[\"clean_answer\"].split(\" \")\n",
    "    split_question = series[\"clean_question\"].split(\" \")\n",
    "    try:\n",
    "        split_answer.remove(\"the\")\n",
    "    except ValueError:\n",
    "        split_answer = split_answer\n",
    "    match_count = 0\n",
    "    if len(split_answer) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        for i in split_answer:\n",
    "            if i in split_question:\n",
    "                match_count += 1\n",
    "                #print(\"Found Match\")\n",
    "        return match_count/len(split_answer)\n",
    "\n",
    "jeopardy[\"answer_in_question\"]=jeopardy.apply(jeopardyrow, axis=1)\n",
    "#print(jeopardy[\"answer_in_question\"].head())\n",
    "mean_answer_in_question = jeopardy[\"answer_in_question\"].mean()\n",
    "print(\"Percent of answers that were in questions: {}\".format(round(mean_answer_in_question*100,2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Percent of questions in answers\n",
    "\n",
    "The share of answers in question on jeopardy only occurs about 6% of the time.  This percentage is low suggesting a winning strategy would not be to answer with part of the question when one does not know the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Investigating recycled questions\n",
    "import time\n",
    "def recycled_questions(wordnum):\n",
    "\n",
    "    #first sort based on air date\n",
    "    jeopardy.sort_values(\"Air Date\", ascending=True)\n",
    "\n",
    "    #create empty list and empty set\n",
    "    question_overlap = []\n",
    "    terms_used = {}\n",
    "    for i,row in jeopardy.iterrows():\n",
    "        split_question = row[\"clean_question\"].split(\" \")\n",
    "        new_question = [word for word in split_question if len(word) >= wordnum]\n",
    "        #print(split_question)\n",
    "        #print(new_question)\n",
    "        match_count = 0\n",
    "        #calculate share of words > 5 that have occurred in Jeopardy rounds\n",
    "        if len(new_question) != 0:\n",
    "            #add all of the words in each row to terms_used list\n",
    "            #print(len(new_question))\n",
    "            for word in new_question:\n",
    "                if word in terms_used:\n",
    "                    match_count += 1\n",
    "                    terms_used[word]+=1\n",
    "                else:\n",
    "                    terms_used[word]=1\n",
    "            question_overlap.append(match_count/len(new_question))\n",
    "        else:\n",
    "            question_overlap.append(0)\n",
    "    jeopardy[\"question_overlap\"] = question_overlap\n",
    "    percentqs = round(jeopardy[\"question_overlap\"].mean()*100,2)\n",
    "    print(\"Percent of questions asked previously (words > {}): {}\".format(wordnum, percentqs))\n",
    "    print(\"Number of words: {}\".format(len(terms_used)))\n",
    "    return percentqs, terms_used\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent of questions asked previously (words > 5): 74.47\n",
      "Number of words: 29500\n",
      "Percent of questions asked previously (words > 7): 61.58\n",
      "Number of words: 19951\n",
      "Percent of questions asked previously (words > 9): 35.22\n",
      "Number of words: 10265\n",
      "Percent of questions asked previously (words > 11): 11.12\n",
      "Number of words: 4681\n"
     ]
    }
   ],
   "source": [
    "terms_used5 = recycled_questions(5)\n",
    "terms_used7 = recycled_questions(7)\n",
    "terms_used9 = recycled_questions(9)\n",
    "terms_used11 = recycled_questions(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAEICAYAAAAZeSDaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x267c434afd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "percentqs_array = [terms_used5[0],terms_used7[0],terms_used9[0],terms_used11[0]]\n",
    "len(percentqs_array)\n",
    "\n",
    "def format_bargraphs(barnames,yval,colorname):\n",
    "    # have maximum length of 10 for the character names\n",
    "    #print(results)\n",
    "    new_bar_values = []\n",
    "    y_pos = np.arange(len(yval))\n",
    "    plt.bar(y_pos, yval, color=colorname)\n",
    "    plt.xticks(y_pos, barnames)\n",
    "    \n",
    "format_bargraphs([5,7,9,11],percentqs_array,\"orange\")\n",
    "plt.ylabel(\"Percent\")\n",
    "plt.title(\"Questions Asked Previously on Jeopardy By Minimum Word Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Percent of questions asked on previous shows\n",
    "\n",
    "Analyzing questions based on critical words (characters > 5) nearly 75% questions have been asked on previous shows this suggests that studying past questions could be helpful for winning on jeopardy.  However, if we restrict the count to words with at least 9 characters if we assume more unique words are typically longer then we find only 35% of questions have asked previously and the number is far smaller when focusing on words that are 11 characters in length.  This suggests that we can identify common words to focus on that could pay off more and have a greater chance of showing up on future jeopardy shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Low value versus high value questions\n",
    "\n",
    "#return indicator of whether a question is high value (worth over 800)\n",
    "def qsvalue(datarow,dollaramt):\n",
    "    return(datarow[\"clean_value\"] >= dollaramt)\n",
    "\n",
    "#count words associated with high value amounts and those associated with low value across the entire sample\n",
    "def valuecount(wordlist,dollaramt):\n",
    "    jeopardy[\"high_value\"]=jeopardy.apply(lambda datarow: qsvalue(datarow,dollaramt),axis=1)\n",
    "    #print(\"High_value questions:\",sum(jeopardy[\"high_value\"]))\n",
    "    low_count = {}\n",
    "    high_count = {}\n",
    "    for i, row in jeopardy.iterrows():\n",
    "        split_question = row[\"clean_question\"].split(\" \")\n",
    "        for word in wordlist:\n",
    "            if word not in low_count:\n",
    "                low_count[word]=0\n",
    "                high_count[word]=0\n",
    "            if word in split_question:\n",
    "                if row[\"high_value\"]==1:\n",
    "                    high_count[word] += 1\n",
    "                else:\n",
    "                    low_count[word] += 1\n",
    "    return(high_count,low_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words with at least 5 letters\n",
      "('these', 1389)\n",
      "('first', 950)\n",
      "('about', 551)\n",
      "('called', 521)\n",
      "('named', 513)\n",
      "('country', 472)\n",
      "('state', 440)\n",
      "('after', 425)\n",
      "('title', 369)\n",
      "('which', 361)\n",
      "('known', 351)\n",
      "('years', 314)\n",
      "('played', 299)\n",
      "('became', 287)\n",
      "('wrote', 286)\n",
      "('before', 267)\n",
      "('novel', 264)\n",
      "('president', 259)\n",
      "('world', 258)\n",
      "('american', 257)\n",
      "\n",
      "\n",
      "Most common words with at least 7 letters\n",
      "('country', 472)\n",
      "('president', 259)\n",
      "('american', 257)\n",
      "('capital', 254)\n",
      "('national', 183)\n",
      "('largest', 179)\n",
      "('british', 166)\n",
      "('meaning', 162)\n",
      "('century', 159)\n",
      "('musical', 153)\n",
      "('company', 150)\n",
      "('between', 145)\n",
      "('reports', 145)\n",
      "('founded', 141)\n",
      "('character', 141)\n",
      "('include', 138)\n",
      "('million', 127)\n",
      "('popular', 119)\n",
      "('because', 111)\n",
      "('through', 104)\n",
      "\n",
      "\n",
      "Most common words with at least 9 letters\n",
      "('president', 259)\n",
      "('character', 141)\n",
      "('university', 98)\n",
      "('published', 92)\n",
      "('introduced', 77)\n",
      "('including', 75)\n",
      "('something', 74)\n",
      "('washington', 67)\n",
      "('california', 66)\n",
      "('government', 63)\n",
      "('presidential', 61)\n",
      "('sometimes', 57)\n",
      "('discovered', 56)\n",
      "('instrument', 56)\n",
      "('political', 54)\n",
      "('according', 54)\n",
      "('adjective', 51)\n",
      "('originally', 51)\n",
      "('international', 49)\n",
      "('secretary', 49)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#now we want to check for common words where the difference is disproportionately high value versus low value\n",
    "#lets focus on the 20 words (out of 20K) that show up the most in the terms_used sample\n",
    "terms5_sort = sorted(terms_used5[1].items(), key=lambda x:x[1], reverse=True)\n",
    "terms7_sort = sorted(terms_used7[1].items(), key=lambda x:x[1], reverse=True)\n",
    "terms9_sort = sorted(terms_used9[1].items(), key=lambda x:x[1], reverse=True)\n",
    "#examine some of the top used words in the list\n",
    "print(\"Most common words with at least 5 letters\")\n",
    "for row in terms5_sort[0:20]:\n",
    "    print(row)\n",
    "print(\"\\n\")\n",
    "print(\"Most common words with at least 7 letters\")\n",
    "for row in terms7_sort[0:20]:\n",
    "    print(row)\n",
    "print(\"\\n\")\n",
    "print(\"Most common words with at least 9 letters\")\n",
    "for row in terms9_sort[0:20]:\n",
    "    print(row)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# look at differences between observed and expected (top 100 words)\n",
    "# words that have the greatest positive difference given that are ranked in the top 50 most common words (over a certain length)\n",
    "def high_value_words(terms,maxwords,dollaramt):\n",
    "    comparison_terms = list(terms)[0:maxwords]\n",
    "    wordlist = []\n",
    "    for word in comparison_terms:\n",
    "        wordlist.append(word[0])\n",
    "    #print(\"Top {} most common words:\".format(maxwords))\n",
    "    #print(comparison_terms)\n",
    "    #print(\"\\n\")\n",
    "    observed_expected = {}\n",
    "    share_highvalue = []\n",
    "    #probably more efficient to send the entire comparison_terms (wordlist) to have efficiency gains\n",
    "    result = valuecount(wordlist,dollaramt)\n",
    "    #print(result)\n",
    "    highvalue_dict = result[0]\n",
    "    lowvalue_dict = result[1]\n",
    "    exp_share = sum(jeopardy[\"high_value\"])/len(jeopardy[\"high_value\"])\n",
    "    share_highvalue_dict = {}\n",
    "    for word in wordlist:\n",
    "        share_highvalue_dict[word] = highvalue_dict[word]/(highvalue_dict[word]+lowvalue_dict[word])\n",
    "        share_highvalue.append(share_highvalue_dict[word])\n",
    "    share_highvalue_dict_sort = sorted(share_highvalue_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "    print(\"Top 10 words (out of {} most common) with the greatest high value share (greater than ${}) :\".format(maxwords,dollaramt))\n",
    "    for row in share_highvalue_dict_sort[0:10]:\n",
    "        print(row)\n",
    "    print(\"\\n\")\n",
    "    print(\"Share of high value questions in sample: {}\".format(exp_share))\n",
    "    print(\"Share (out of {} most common) words that have high value questions (greater than ${}): {}\".\n",
    "          format(maxwords, dollaramt, np.mean(share_highvalue)))\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    return(sum(jeopardy[\"high_value\"]), highvalue_dict, lowvalue_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words (out of 50 most common) with the greatest high value share (greater than $500) :\n",
      "('french', 0.7053941908713693)\n",
      "('greek', 0.7025316455696202)\n",
      "('whose', 0.6727272727272727)\n",
      "('known', 0.6628242074927954)\n",
      "('island', 0.6618357487922706)\n",
      "('meaning', 0.65625)\n",
      "('great', 0.6464646464646465)\n",
      "('means', 0.6439024390243903)\n",
      "('house', 0.6392405063291139)\n",
      "('named', 0.6354581673306773)\n",
      "\n",
      "\n",
      "Share of high value questions in sample: 0.5709285464273214\n",
      "Share (out of 50 most common) words that have high value questions (greater than $500): 0.5933361100400942\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Top 10 words (out of 50 most common) with the greatest high value share (greater than $500) :\n",
      "('reports', 0.7172413793103448)\n",
      "('italian', 0.7142857142857143)\n",
      "('another', 0.6842105263157895)\n",
      "('meaning', 0.65625)\n",
      "('university', 0.6559139784946236)\n",
      "('college', 0.6282051282051282)\n",
      "('english', 0.6276595744680851)\n",
      "('original', 0.6266666666666667)\n",
      "('something', 0.625)\n",
      "('against', 0.6125)\n",
      "\n",
      "\n",
      "Share of high value questions in sample: 0.5709285464273214\n",
      "Share (out of 50 most common) words that have high value questions (greater than $500): 0.5777801812472597\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Top 10 words (out of 50 most common) with the greatest high value share (greater than $500) :\n",
      "('developed', 0.7435897435897436)\n",
      "('playwright', 0.7333333333333333)\n",
      "('nicknamed', 0.696969696969697)\n",
      "('secretary', 0.6956521739130435)\n",
      "('peninsula', 0.6875)\n",
      "('americans', 0.6756756756756757)\n",
      "('adjective', 0.6666666666666666)\n",
      "('university', 0.6559139784946236)\n",
      "('sometimes', 0.6545454545454545)\n",
      "('religious', 0.6486486486486487)\n",
      "\n",
      "\n",
      "Share of high value questions in sample: 0.5709285464273214\n",
      "Share (out of 50 most common) words that have high value questions (greater than $500): 0.5662181313274359\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Top 10 words (out of 50 most common) with the greatest high value share (greater than $800) :\n",
      "('french', 0.5975103734439834)\n",
      "('greek', 0.5569620253164557)\n",
      "('island', 0.5265700483091788)\n",
      "('means', 0.5219512195121951)\n",
      "('whose', 0.5151515151515151)\n",
      "('meaning', 0.50625)\n",
      "('known', 0.5014409221902018)\n",
      "('named', 0.5)\n",
      "('author', 0.5)\n",
      "('century', 0.5)\n",
      "\n",
      "\n",
      "Share of high value questions in sample: 0.43572178608930445\n",
      "Share (out of 50 most common) words that have high value questions (greater than $800): 0.45899411416110975\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Top 10 words (out of 50 most common) with the greatest high value share (greater than $800) :\n",
      "('reports', 0.6275862068965518)\n",
      "('italian', 0.6122448979591837)\n",
      "('something', 0.5277777777777778)\n",
      "('university', 0.5161290322580645)\n",
      "('another', 0.5157894736842106)\n",
      "('ancient', 0.5138888888888888)\n",
      "('meaning', 0.50625)\n",
      "('century', 0.5)\n",
      "('original', 0.49333333333333335)\n",
      "('english', 0.48936170212765956)\n",
      "\n",
      "\n",
      "Share of high value questions in sample: 0.43572178608930445\n",
      "Share (out of 50 most common) words that have high value questions (greater than $800): 0.44449222642019476\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Top 10 words (out of 50 most common) with the greatest high value share (greater than $800) :\n",
      "('secretary', 0.6086956521739131)\n",
      "('playwright', 0.6)\n",
      "('adjective', 0.5882352941176471)\n",
      "('developed', 0.5641025641025641)\n",
      "('newspaper', 0.5428571428571428)\n",
      "('americans', 0.5405405405405406)\n",
      "('performed', 0.5333333333333333)\n",
      "('peninsula', 0.53125)\n",
      "('something', 0.5277777777777778)\n",
      "('sometimes', 0.5272727272727272)\n",
      "\n",
      "\n",
      "Share of high value questions in sample: 0.43572178608930445\n",
      "Share (out of 50 most common) words that have high value questions (greater than $800): 0.4306985431048085\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#note the iteration process is quite slow.  Probably would be better to tokenize it or place into matrix so we don't have to \n",
    "#go through all the loops\n",
    "share5_highvalue500 = high_value_words(terms5_sort,50,500)\n",
    "share7_highvalue500 = high_value_words(terms7_sort,50,500)\n",
    "share9_highvalue500 = high_value_words(terms9_sort,50,500)\n",
    "share5_highvalue800 = high_value_words(terms5_sort,50,800)\n",
    "share7_highvalue800 = high_value_words(terms7_sort,50,800)\n",
    "share9_highvalue800 = high_value_words(terms9_sort,50,800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#applying the Chi-squared test to high and low-counts\n",
    "import scipy.stats as stats\n",
    "\n",
    "def compute_chi_squared(highvaluevars):\n",
    "\n",
    "    #expected share in distribution\n",
    "    exp_share = highvaluevars[0]/len(jeopardy[\"high_value\"])  #this is the share that is in sample population\n",
    "    highvalue_count = highvaluevars[1]\n",
    "    lowvalue_count = highvaluevars[2]\n",
    "\n",
    "    exp_high = []\n",
    "    exp_low = []\n",
    "    exp = []\n",
    "    obs_high = []\n",
    "    obs_low = []\n",
    "    obs = []\n",
    "    for word, v in highvalue_count.items():\n",
    "        #total_prop is share of question in total questions\n",
    "        exp.append([exp_share*(highvalue_count[word]+lowvalue_count[word]),(1-exp_share)*(highvalue_count[word]+lowvalue_count[word])])\n",
    "        obs.append([highvalue_count[word],lowvalue_count[word]])\n",
    "        exp_high.append(exp_share*(highvalue_count[word]+lowvalue_count[word]))\n",
    "        exp_low.append((1-exp_share)*(highvalue_count[word]+lowvalue_count[word]))\n",
    "        #print([highvalue_count,exp_share*(highvalue_count[word]+lowvalue_count[word])])\n",
    "        obs_high.append(highvalue_count[word])\n",
    "        obs_low.append(lowvalue_count[word])\n",
    "\n",
    "    #create chi-squared for all high value questions\n",
    "    print(\"Chi-squared for high value counts: {}\".format(stats.chisquare(obs_high,exp_high)))\n",
    "    print(\"Chi-squared for low value counts: {}\".format(stats.chisquare(obs_low,exp_low)))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-squared for high value counts: Power_divergenceResult(statistic=57.068459647104412, pvalue=0.20025976710910895)\n",
      "Chi-squared for low value counts: Power_divergenceResult(statistic=75.936099784481797, pvalue=0.008115371881753243)\n",
      "\n",
      "\n",
      "Chi-squared for high value counts: Power_divergenceResult(statistic=29.26905251697594, pvalue=0.98866918699306039)\n",
      "Chi-squared for low value counts: Power_divergenceResult(statistic=38.94581536404047, pvalue=0.84759613321258875)\n",
      "\n",
      "\n",
      "Chi-squared for high value counts: Power_divergenceResult(statistic=33.944812665944788, pvalue=0.94980611116822877)\n",
      "Chi-squared for low value counts: Power_divergenceResult(statistic=45.16744796873995, pvalue=0.62925117187710433)\n",
      "\n",
      "\n",
      "Chi-squared for high value counts: Power_divergenceResult(statistic=72.977970498911858, pvalue=0.014749442924201302)\n",
      "Chi-squared for low value counts: Power_divergenceResult(statistic=56.351797512407416, pvalue=0.21915352706695418)\n",
      "\n",
      "\n",
      "Chi-squared for high value counts: Power_divergenceResult(statistic=53.681382040520319, pvalue=0.29961492790159849)\n",
      "Chi-squared for low value counts: Power_divergenceResult(statistic=41.45144555614479, pvalue=0.76953245872455323)\n",
      "\n",
      "\n",
      "Chi-squared for high value counts: Power_divergenceResult(statistic=45.751525179913706, pvalue=0.60561359417279226)\n",
      "Chi-squared for low value counts: Power_divergenceResult(statistic=35.328204733519527, pvalue=0.92873801886054308)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compute_chi_squared(share5_highvalue500)\n",
    "compute_chi_squared(share7_highvalue500)\n",
    "compute_chi_squared(share9_highvalue500)\n",
    "compute_chi_squared(share5_highvalue800)\n",
    "compute_chi_squared(share7_highvalue800)\n",
    "compute_chi_squared(share9_highvalue800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the variation between high and low value counts\n",
    "\n",
    "Our analysis shows that if we use a thresshold of 500 there is little differences in the times that top 50 most common are associated with high paying questions versus low-paying questions compared to what is expected in the full set of questions using both thresshold.  However, increasing the thresshold for high-value to 800 it is observed that there could be a statistically significant difference in the share of high value questions compared to the norm where the character length of words is at least 5 or 7 characters in length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "We found that there is frequent recycling of questions on Jeopardy with 75% of words having shown up on multiple Jeopardy shows.  \n",
    "This led us to identify the top 50 common words (and associated questions) that come up frequently and suggests areas to concentrate on to study to get more questions right on Jeopardy.  However, by focusing on these common questions and analyzing the share that were associated with higher values (on average) we were able to narrow down the set of questions that both commonly come up and likewise are more likely to be associated with higher values.  Examining these words they tend to to be associated more likely in the category of historical, political knowledge suggesting that focusing on these types of questions could lead to greater pay-offs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
